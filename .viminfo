# This viminfo file was generated by Vim 8.2.
# You may edit it if you're careful!

# Viminfo version
|1,4

# Value of 'encoding' when this file was written
*encoding=latin1


# hlsearch on (H) or off (h):
~h
# Command Line History (newest to oldest):
:wq
|2,0,1716737146,,"wq"
:w
|2,0,1716737145,,"w"
:q
|2,0,1716735220,,"q"

# Search String History (newest to oldest):

# Expression History (newest to oldest):

# Input Line History (newest to oldest):

# Debug Line History (newest to oldest):

# Registers:
"1	LINE	0
	from langchain_community.llms import Ollama
	import streamlit as st
	from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
	
	# llm = Ollama(model="phi:latest", base_url="http://ollama-container:11434", verbose=True)
	llm = Ollama(model="llama3", base_url="http://ollama-container:11434", verbose=True)
	
	def sendPrompt(prompt):
	    global llm
	    response = llm.invoke(prompt)
	    return response
	
	285
<st.set_page_config(page_title='My Little RAG', layout = 'wide', page_icon = 'https://media.licdn.com/dms/image/C5603AQFFBPEfIzXcgg/profile-displayphoto-shrink_200_200/0/1604552403158?e=2147483647&v=beta&t=mWcPkWfIc4XZ17UBffkKw8Q9D9P0RweCfhExkb_JQAM', initial_sidebar_state = 'auto')
	st.title("Karl's Little Chatbot with a Database")
	if "messages" not in st.session_state.keys(): 
	    st.session_state.messages = [
	        {"role": "assistant", "content": "Ask me a question !"}
	    ]
	
	if prompt := st.chat_input("Your question"): 
	    st.session_state.messages.append({"role": "user", "content": prompt})
	
	for message in st.session_state.messages: 
	    with st.chat_message(message["role"]):
	        st.write(message["content"])
	        
	if st.session_state.messages[-1]["role"] != "assistant":
	    with st.chat_message("assistant"):
	        with st.spinner("Thinking..."):
	            response = sendPrompt(prompt)
	            print(response)
	            st.write(response)
	            message = {"role": "assistant", "content": response}
	            st.session_state.messages.append(message) 
|3,0,1,1,34,0,1716736337,"from langchain_community.llms import Ollama","import streamlit as st","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler","","# llm = Ollama(model=\"phi:latest\", base_url=\"http://ollama-container:11434\", verbose=True)","llm = Ollama(model=\"llama3\", base_url=\"http://ollama-container:11434\", verbose=True)","","def sendPrompt(prompt):","    global llm","    response = llm.invoke(prompt)",>21
|<"    return response","","st.set_page_config(page_title='My Little RAG', layout = 'wide', page_icon = 'https://media.licdn.com/dms/image/C5603AQFFBPEfIzXcgg/profile-displayphoto-shrink_200_200/0/1604552403158?e=2147483647&v=beta&t=mWcPkWfIc4XZ17UBffkKw8Q9D9P0RweCfhExkb_JQAM', initial_sidebar_state = 'auto')","st.title(\"Karl's Little Chatbot with a Database\")","if \"messages\" not in st.session_state.keys(): ","    st.session_state.messages = [",>73
|<"        {\"role\": \"assistant\", \"content\": \"Ask me a question !\"}","    ]","","if prompt := st.chat_input(\"Your question\"): ","    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})","","for message in st.session_state.messages: ","    with st.chat_message(message[\"role\"]):","        st.write(message[\"content\"])","        ","if st.session_state.messages[-1][\"role\"] != \"assistant\":","    with st.chat_message(\"assistant\"):",>43
|<"        with st.spinner(\"Thinking...\"):","            response = sendPrompt(prompt)","            print(response)","            st.write(response)","            message = {\"role\": \"assistant\", \"content\": response}","            st.session_state.messages.append(message) "
""-	CHAR	0
	from 
|3,1,36,0,1,0,1716737055,"from "

# File marks:
'0  7  23  ~/README.md
|4,48,7,23,1716737146,"~/README.md"
'1  7  0  ~/README.md
|4,49,7,0,1716737067,"~/README.md"
'2  26  42  ~/app.py
|4,50,26,42,1716736982,"~/app.py"
'3  1  0  ~/app.py
|4,51,1,0,1716735220,"~/app.py"
'4  1  0  ~/app.py
|4,52,1,0,1716735220,"~/app.py"

# Jumplist (newest first):
-'  7  23  ~/README.md
|4,39,7,23,1716737146,"~/README.md"
-'  7  0  ~/README.md
|4,39,7,0,1716737067,"~/README.md"
-'  1  0  ~/README.md
|4,39,1,0,1716737014,"~/README.md"
-'  1  0  ~/README.md
|4,39,1,0,1716737014,"~/README.md"
-'  26  42  ~/app.py
|4,39,26,42,1716736982,"~/app.py"
-'  26  42  ~/app.py
|4,39,26,42,1716736982,"~/app.py"
-'  26  42  ~/app.py
|4,39,26,42,1716736982,"~/app.py"
-'  26  42  ~/app.py
|4,39,26,42,1716736982,"~/app.py"
-'  1  0  ~/app.py
|4,39,1,0,1716736337,"~/app.py"
-'  1  0  ~/app.py
|4,39,1,0,1716736337,"~/app.py"
-'  1  0  ~/app.py
|4,39,1,0,1716736337,"~/app.py"
-'  1  0  ~/app.py
|4,39,1,0,1716736337,"~/app.py"
-'  1  0  ~/app.py
|4,39,1,0,1716735220,"~/app.py"
-'  1  0  ~/app.py
|4,39,1,0,1716735220,"~/app.py"
-'  1  0  ~/app.py
|4,39,1,0,1716735220,"~/app.py"
-'  1  0  ~/app.py
|4,39,1,0,1716735220,"~/app.py"

# History of marks within files (newest to oldest):

> ~/README.md
	*	1716737145	0
	"	7	23
	^	7	24
	.	7	23
	+	3	65
	+	6	0
	+	9	31
	+	7	23

> ~/app.py
	*	1716736339	0
	"	26	42
	^	26	43
	.	26	42
	+	1	22
	+	26	42
